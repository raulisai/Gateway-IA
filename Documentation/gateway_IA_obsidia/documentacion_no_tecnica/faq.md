---
tags:
  - faq
  - help
  - support
type: business
title: FAQ
created: '2026-01-11'
---
# â“ FAQ - Preguntas Frecuentes

> Respuestas a las preguntas mÃ¡s comunes sobre el LLM Gateway.

## General

### Â¿QuÃ© es el LLM Gateway?
Es una plataforma que se sitÃºa entre tu aplicaciÃ³n y los proveedores de LLM (OpenAI, Anthropic, etc.). Analiza cada request y automÃ¡ticamente selecciona el modelo mÃ¡s adecuado para optimizar costos sin sacrificar calidad.

### Â¿CÃ³mo ahorra dinero?
```mermaid
graph LR
    subgraph "Sin Gateway"
        A1[Todo va a GPT-4] --> C1[ðŸ’°ðŸ’°ðŸ’°]
    end
    
    subgraph "Con Gateway"
        A2[Tareas simples] --> B2[GPT-3.5] --> C2[ðŸ’°]
        A3[Tareas complejas] --> B3[GPT-4] --> C3[ðŸ’°ðŸ’°ðŸ’°]
    end
```

El gateway detecta que muchas tareas no necesitan el modelo mÃ¡s caro y las enruta a modelos mÃ¡s econÃ³micos.

### Â¿CuÃ¡nto puedo ahorrar?
En promedio, los usuarios reportan entre **30-50% de ahorro** en costos de LLM. El ahorro exacto depende de tu mix de uso.

---

## TÃ©cnico

### Â¿CÃ³mo integro el gateway?
Es tan simple como cambiar la URL base de tu cliente OpenAI:

```python
# Antes
client = OpenAI(api_key="sk-...")

# DespuÃ©s
client = OpenAI(
    api_key="gw-your-gateway-key",
    base_url="https://api.llm-gateway.com/v1"
)
```

### Â¿Es compatible con mi cÃ³digo existente?
SÃ­. La API es 100% compatible con el formato de OpenAI. No necesitas cambiar nada mÃ¡s que la URL base y la API key.

### Â¿QuÃ© providers soportan?
Actualmente:
- âœ… OpenAI (GPT-4, GPT-3.5, etc.)
- âœ… Anthropic (Claude 3)
- ðŸ”œ Google (Gemini)
- ðŸ”œ Groq
- ðŸ”œ Local models (Ollama)

### Â¿Puedo forzar un modelo especÃ­fico?
SÃ­. Si pasas el parÃ¡metro `model` en tu request, el gateway respetarÃ¡ tu elecciÃ³n y no harÃ¡ auto-routing.

### Â¿Soporta streaming?
SÃ­, en el tier Pro y Enterprise.

---

## Seguridad

### Â¿QuÃ© pasa con mis API keys de providers?
Tus API keys se almacenan encriptadas usando AES-256. Nunca almacenamos las keys en texto plano y nunca las vemos.

### Â¿Mis datos pasan por sus servidores?
SÃ­, pero:
- No almacenamos el contenido de los requests/responses
- Solo guardamos metadata para analytics (tokens, costos, latencia)
- Puedes self-hostear si prefieres control total

### Â¿Son SOC2 compliant?
Estamos en proceso de certificaciÃ³n. Para Enterprise, contacta a sales para discutir requerimientos especÃ­ficos.

---

## Billing

### Â¿CÃ³mo funciona el pricing?
El gateway NO cobra por uso de tokens. Solo cobramos una suscripciÃ³n mensual basada en features y lÃ­mites de requests.

Tus costos de LLM siguen yendo directamente a los providers (OpenAI, Anthropic, etc.) usando TUS API keys.

### Â¿Hay free tier?
SÃ­. El free tier incluye:
- 10,000 requests/mes
- 2 gateway keys
- 2 providers
- 7 dÃ­as de analytics

### Â¿Puedo cancelar en cualquier momento?
SÃ­, sin penalidad ni preguntas.

---

## Troubleshooting

### Mi request falla con 401
Verifica que:
1. Tu gateway key es correcta
2. El formato es `Authorization: Bearer gw_xxxxx`
3. La key no ha sido revocada

### La latencia es alta
Posibles causas:
1. El modelo seleccionado tiene alta latencia inherente
2. Rate limiting del provider
3. Considera habilitar cache para requests repetitivos

### No veo analytics
- Los analytics pueden tomar hasta 1 minuto en actualizarse
- Verifica que tus requests estÃ¡n usando una gateway key vÃ¡lida
- El free tier solo retiene 7 dÃ­as de datos

---

## Contacto

### Â¿CÃ³mo consigo ayuda?
- **Free**: Community Discord
- **Pro**: Email support (respuesta <24h)
- **Enterprise**: Slack dedicado + Account manager

### Â¿Puedo solicitar features?
Â¡Absolutamente! EnvÃ­a tus ideas a feedback@llm-gateway.com o vota por features existentes en nuestro roadmap pÃºblico.

---

*Ver tambiÃ©n: [[vision-producto|VisiÃ³n del Producto]] | [[../documentacion/troubleshooting|Troubleshooting TÃ©cnico]]*
